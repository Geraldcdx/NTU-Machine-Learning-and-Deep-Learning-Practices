{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "from rouge_score import rouge_scorer #https://pypi.org/project/rouge-score/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install rouge-score\n",
    "#pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset to test\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents\n",
    "def prepareDataset():\n",
    "    f = open(\"deu.txt\", \"r\", encoding = 'utf-8')\n",
    "    systematic_samples = [\n",
    "                          (english_german[0],english_german[1]) \n",
    "                          for index, english_german in enumerate(to_lines(f.read(2500000))) \n",
    "                          if index%100 == 0  #choose every 100\n",
    "                         ]\n",
    "    print('Number Of Samples: ', len(systematic_samples))\n",
    "    return systematic_samples\n",
    "\n",
    "def generate_rouge():\n",
    "    #set up rouge scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)#rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    #prepare the dataset to test\n",
    "    english_german = prepareDataset()\n",
    "    precision_fscore_fmeasure=[[],[],[]]\n",
    "    for (english,germanTranslation) in english_german: \n",
    "        ##TODO: Change your translator in the bottom code\n",
    "        germanOutput = GoogleTranslator(source='auto', target='de').translate(english) \n",
    "        scores = scorer.score(germanOutput,germanTranslation)['rouge1']\n",
    "        precision_fscore_fmeasure[0].append(scores.precision)\n",
    "        precision_fscore_fmeasure[1].append(scores.recall)\n",
    "        precision_fscore_fmeasure[2].append(scores.fmeasure)\n",
    "    return precision_fscore_fmeasure\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Samples:  209\n"
     ]
    }
   ],
   "source": [
    "scores = generate_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of precision:  0.7040631882737145\n",
      "Mean of recall:  0.7275461380724538\n",
      "Mean of fmeasure:  0.711829956327564\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of precision: \",np.mean(scores[0]))\n",
    "print(\"Mean of recall: \",np.mean(scores[1]))\n",
    "print(\"Mean of fmeasure: \",np.mean(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
