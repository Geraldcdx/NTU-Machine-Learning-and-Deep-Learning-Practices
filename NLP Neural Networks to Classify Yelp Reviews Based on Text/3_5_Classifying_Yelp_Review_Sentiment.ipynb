{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Yelp Reviews (Classifying Sentiment of Restaurant Reviews, chapter 3) \n",
    "\n",
    "### Source: Chapter 3, Natural Language Processing with Pytorch. (2019). Delip Rao and Brian McMahan. Oâ€™Reilly: source code available on https://github.com/joosthub/PyTorchNLPBook\n",
    "\n",
    "### PyTorch tutorial: refer to https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import inflect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#added import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk import word_tokenize\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "correct_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop for finding correct spellings\n",
    "# # based on edit distance and\n",
    "# # printing the correct words\n",
    "\n",
    "# #Define the list of incorrect_words for which we need the correct spellings. \n",
    "# #Then run a loop for each word in the incorrect words list to \n",
    "# #calculate the Edit distance of the incorrect word with each correct spelling word having the same initial letter.\n",
    "# #Then sort them in ascending order so the shortest distance is on top and extract the word corresponding to it and print it.\n",
    "# def incorrect_wordCheck(word):\n",
    "#     if word in correct_words:\n",
    "#         return word\n",
    "#     else:\n",
    "#         temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "#         try:\n",
    "#             word = sorted(temp, key = lambda val:val[0])[0][1]\n",
    "#         except:\n",
    "#             return word #unable to find a correct word\n",
    "#         return word\n",
    "# print(incorrect_wordCheck('happpy'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingText(myString):\n",
    "    myString = re.sub(r'\\d+', '', myString) #remove numbers\n",
    "    #trantab = str.maketrans(string.punctuation, \" \"*32) #remove punctuation\n",
    "    #myString = myString.translate(trantab)\n",
    "    #myString = re.sub(r'\\s+', ' ', myString) # \\s+ matches a whitespace character whose length is 1 or more than 1.\n",
    "    myString = myString.strip() # removing trailing whitespaces\n",
    "    myString = re.sub(r'\\[[^]]*\\]', '', myString) # remove between square brackets # [^]]* : match anything except ']'\n",
    "    return myString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx         # _token_to_idx: {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "                                                  # _idx_to_token: {0:'<UNK>', 1:'apple', 2:'banana', ....., 10000:'zzz'}\n",
    "\n",
    "        self._idx_to_token = \\\n",
    "        {idx: wnl.lemmatize(contractions.fix((re.sub(r'(?!<\\d)\\.(?!\\d)|[^\\s\\w.]',token.lower())))) \\\n",
    "        for token, idx in self._token_to_idx.items() if not stopwords.words('english')} \n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "              \n",
    "    def add_token(self, token): #added: modify to skip token\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        \n",
    "        token = re.sub(r'(?!<\\d)\\.(?!\\d)|[^\\s\\w.]', '', token.lower())  #added\n",
    "        if token in stopwords.words('english'):#added\n",
    "            return -1\n",
    "        token = wnl.lemmatize(contractions.fix(token))\n",
    "        if token in self._token_to_idx: \n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:                   # if unk_token is defined, unknown words are handled\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "    \n",
    "    def show(self):#added\n",
    "        print(self._token_to_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers; {'negative':0, 'positive':1}\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab     # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "        self.rating_vocab = rating_vocab     # {'negative':0, 'positive':1}\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Create a collapsed one-hot vector for the review\n",
    "        \n",
    "        Args:\n",
    "            review (str): the review \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding   \n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        #review = processingText(review) #add text preprocessing\n",
    "        for token in review.split(\" \"):\n",
    "            \n",
    "            token = re.sub(r'(?!<\\d)\\.(?!\\d)|[^\\s\\w.]', '', token.lower()) #added regex\n",
    "            token = wnl.lemmatize(contractions.fix(token))\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "            else:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] +=1\n",
    "        #implemented a word vector\n",
    "        return one_hot  # E.g., \"Unfortunately, the frustration of being Dr. Go...\" -> [0, 0, 1, 0, 1, ....., 0, 0]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewVectorizer\n",
    "            review_df (pandas.DataFrame): the review dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        #print(\"This is the review_df\", review_df)\n",
    "        \n",
    "        review_vocab = Vocabulary(add_unk=True)   # create an instance of Vocabulary class\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # Add ratings\n",
    "        for rating in sorted(set(review_df.rating)):  \n",
    "            rating_vocab.add_token(rating)            # {'negative':0, 'positive':1}\n",
    "\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            #review = processingText(review) #add text preprocessing\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation: #and not stopwords.words('english'): #added\n",
    "                    word = re.sub(r'(?!<\\d)\\.(?!\\d)|[^\\s\\w.]', '', word.lower()) #added regex\n",
    "                    word = wnl.lemmatize(contractions.fix(word))\n",
    "                    word_counts[word] += 1 #added\n",
    "        \n",
    "        word_counts = {word: count for word, count in word_counts.items() if word not in stopwords.words('english')}\n",
    "        for word, count in word_counts.items():#for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)          # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "        #review_vocab.show() #added\n",
    "            \n",
    "        return cls(review_vocab, rating_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(\"Loading dataset and creating vectorizer\")\n",
    "# # create dataset and vectorizer\n",
    "# dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv, args.frequency_cutoff) \n",
    "\n",
    "# vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab), hidden_dim=args.hidden_dim)\n",
    "#dataset._vectorizer.review_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv, frequency_cutoff):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewDataset\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df, frequency_cutoff))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,           #  e.g., { 'x_data': [0, 0, 1, 0, 1, ....., 0, 0],\n",
    "                'y_target': rating_index}          #          'y_target': 0  }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  # the floor division // rounds the result down to the nearest whole number\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    # drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. \n",
    "    # If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n",
    "    # need to use sampler option for balanced data: \n",
    "    # https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():  # name: x_data & y_target\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "            hidden_dim   (int): the size of hidden dimension\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=100, bias = True) \n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=1, bias = True)\n",
    "        #self.fc3 = nn.Linear(in_features=15, out_features=5, bias = True)\n",
    "        #self.bn1 = nn.BatchNorm1d(20)\n",
    "        #self.bn2 = nn.BatchNorm1d(15)\n",
    "        #self.bn3 = nn.BatchNorm1d(15)\n",
    "        self.dpout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the classifier    \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be [batch, num_features]\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be [batch]\n",
    "        \"\"\"\n",
    "        intermediate = self.fc1(x_in)            # [batch, num_features] -> [batch, hidden_dim]\n",
    "        #intermediate = self.bn1(intermediate)\n",
    "        intermediate = F.relu(intermediate)      # [batch, hidden_dim]\n",
    "        intermediate = self.dpout(intermediate)\n",
    "        #intermediate = self.i1(intermediate)\n",
    "        #intermediate = self.bn2(intermediate)\n",
    "        #intermediate = F.relu(intermediate)\n",
    "        #intermediate = self.dpout(intermediate)\n",
    "        #intermediate = self.i2(intermediate)\n",
    "        y_out = self.fc2(intermediate)           # [batch, hidden_dim] -> [batch, out_features]\n",
    "        \n",
    "               \n",
    "        return torch.sigmoid(y_out).squeeze()    # [batch, 1] -> [batch] (e.g., [0.3, 0.1, 0.7, 0.8, ..., 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (y_pred>0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()  # item() to get a Python number from a tensor containing a single value\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=0,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    # No Model hyper parameters\n",
    "    hidden_dim=20,\n",
    "    # Training hyper parameters\n",
    "    batch_size=128, #was 128\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs ; creat dirs if they don't exist\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset and creating vectorizer\n",
      "preparing vectorizer\n",
      "Preparing classifier\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset and creating vectorizer\")\n",
    "# create dataset and vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv, args.frequency_cutoff) \n",
    "print(\"preparing vectorizer\")\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "print(\"Preparing classifier\")\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab), hidden_dim=args.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<Vocabulary(size=34034)>', '<Vocabulary(size=2)>')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(vectorizer.review_vocab), str(vectorizer.rating_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "1. lr_scheduler.ReduceLROnPlateau(): Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a â€˜patienceâ€™ number of epochs, the learning rate is reduced.(https://pytorch.org/docs/stable/optim.html)\n",
    "> - mode (str) â€“ One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: â€˜minâ€™.<br>\n",
    ">- factor (float) â€“ Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.<br>\n",
    ">- patience (int) â€“ Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasnâ€™t improved then. Default: 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     30
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b14c5b29f44e1b68bafb9e89609a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ba9b0a2ad04560911d8eb3657bcdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=61.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca1534a90ab4e1e85ad76935bba76b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=13.0, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "#AdamW is slightly better                           # was 0.001\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=0.001) #tune from 0.001, , weight_decay=0.001\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.1,\n",
    "                                                 patience=10) # Reduce learning rate when a metric has stopped improving.\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)  # progress bar\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            \n",
    "            \n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())  # [batch, num_features] -> [batch]\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                  acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "        \n",
    "        train_state['train_loss'].append(running_loss)  # train_loss for each epoch\n",
    "        train_state['train_acc'].append(running_acc)    # train_acc for each epoch\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)  # val_loss for each epoch\n",
    "        train_state['val_acc'].append(running_acc)    # val_acc for each epoch\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])  # adjust learning rate\n",
    "\n",
    "        train_bar.n = 0  # Number of finished iterations\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 1   # reset number of finished iterations\n",
    "        val_bar.n = 1\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZxcZZn3/883ezr7BkJC0hF5FLI3TQATIggTlkE2URLDDIsQ2TcdZYR54KcDKrLpTx8eI8jo0AYZMIKKEcSwSoAOSyCgBiHEJhE6CyFJJ5CG6/njnO5UOlWdqk53V1f6+369zqvq7FdVV5/rnPu+z30UEZiZmeWrS7EDMDOz0uLEYWZmBXHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcO22mSukraIGlkay5bTJI+JqnV26pLOkLSsozxv0g6JJ9lW7CvWyV9vaXrm+XSrdgBWPuTtCFjtAx4D/ggHf9SRFQVsr2I+ADo29rLdgYR8fHW2I6ks4BTI+LQjG2f1RrbNmvKiaMTiojGA3d6RntWRPwh1/KSukVEfXvEZrYj/j0Wn4uqbDuS/lPSLyTNlbQeOFXSwZIWSnpH0kpJ35fUPV2+m6SQVJ6O35HO/52k9ZKelDS60GXT+UdL+qukdZL+f0lPSDo9R9z5xPglSa9KWivp+xnrdpV0k6TVkv4GHNXM93OlpDubTPuhpBvT92dJeiX9PH9LrwZybatG0qHp+zJJ/53GtgTYP8t+X0u3u0TScen0ccAPgEPSYsBVGd/t1Rnrn5N+9tWSfiVpj3y+m0K+54Z4JP1B0hpJ/5D01Yz9/Ef6nbwrqVrSntmKBSU93vB3Tr/PR9P9rAGulLSPpAXpZ1mVfm8DMtYflX7G2nT+9yT1SmPeN2O5PSTVSRqS6/NaFhHhoRMPwDLgiCbT/hN4H/gMyclFb+AA4ECSq9SPAn8FLkiX7wYEUJ6O3wGsAiqB7sAvgDtasOxuwHrg+HTeZcAW4PQcnyWfGO8FBgDlwJqGzw5cACwBRgBDgEeTf4+s+/kosAHok7Htt4HKdPwz6TICPg1sAsan844AlmVsqwY4NH1/PfAwMAgYBbzcZNnPA3ukf5MvpDHsns47C3i4SZx3AFen76enMU4EegH/B/hjPt9Ngd/zAOAt4GKgJ9AfmJzO+3fgBWCf9DNMBAYDH2v6XQOPN/yd089WD5wLdCX5Pf4v4HCgR/o7eQK4PuPzvJR+n33S5aek8+YA12Ts58vAvGL/H5baUPQAPBT5B5A7cfxxB+t9Bfif9H22ZPB/M5Y9DnipBcueCTyWMU/ASnIkjjxjPChj/i+Br6TvHyUpsmuYd0zTg1mTbS8EvpC+Pxr4azPL/gY4P33fXOJYnvm3AM7LXDbLdl8C/jl9v6PE8VPg2ox5/UnqtUbs6Lsp8Hv+F6A6x3J/a4i3yfR8EsdrO4jhZOCZ9P0hwD+ArlmWmwK8Digdfx44qbX/r3b1wUVVlsvfM0ckfULSb9Oih3eBbwBDm1n/Hxnv62i+QjzXsntmxhHJf3pNro3kGWNe+wLeaCZegJ8DM9P3XwAaGxRIOlbSU2lRzTskZ/vNfVcN9mguBkmnS3ohLW55B/hEntuF5PM1bi8i3gXWAsMzlsnrb7aD73kv4NUcMexFkjxaounv8SOS7pL0ZhrDfzWJYVkkDTG2ERFPkFy9TJU0FhgJ/LaFMXVaThyWS9OmqD8iOcP9WET0B/43yRVAW1pJckYMgCSx7YGuqZ2JcSXJAafBjpoL/wI4QtIIkqK0n6cx9gbuBr5FUow0EHggzzj+kSsGSR8FbiEprhmSbvfPGdvdUdPhFSTFXw3b60dSJPZmHnE11dz3/Hdg7xzr5Zq3MY2pLGPaR5os0/TzfYekNeC4NIbTm8QwSlLXHHH8DDiV5Ororoh4L8dyloMTh+WrH7AO2JhWLn6pHfb5G6BC0mckdSMpNx/WRjHeBVwiaXhaUfq15haOiLdIilNuB/4SEUvTWT1Jyt1rgQ8kHUtSFp9vDF+XNFDJfS4XZMzrS3LwrCXJoWeRXHE0eAsYkVlJ3cRc4IuSxkvqSZLYHouInFdwzWjue74PGCnpAkk9JPWXNDmddyvwn5L2VmKipMEkCfMfJI0wukqaTUaSayaGjcA6SXuRFJc1eBJYDVyrpMFBb0lTMub/N0nR1hdIkogVyInD8vVl4DSSyuofkZxxt6n04HwKcCPJgWBv4DmSM83WjvEW4CHgReAZkquGHfk5SZ3FzzNifge4FJhHUsF8MkkCzMdVJFc+y4DfkXFQi4jFwPeBp9NlPgE8lbHug8BS4C1JmUVODevPJylSmpeuPxKYlWdcTeX8niNiHfBPwGdJKuP/Cnwqnf1d4Fck3/O7JBXVvdIiyLOBr5M0lPhYk8+WzVXAZJIEdh9wT0YM9cCxwL4kVx/LSf4ODfOXkfyd34+IPxX42Y2tFURmHV5a9LACODkiHit2PFa6JP2MpML96mLHUop8A6B1aJKOIil62EzSnLOe5KzbrEXS+qLjgXHFjqVUuajKOrqpwGskRRhHASe4MtNaStK3SO4luTYilhc7nlLloiozMyuIrzjMzKwgnaKOY+jQoVFeXl7sMMzMSsaiRYtWRUTW5u9tljgk/YSkSdzbETE2nTaYpOleOUmTw89HxNr0xq7vkXTzUEfS1cCzWba5P8kdor2B+4GLI4+ytvLycqqrq1vhU5mZdQ6Scvae0JZFVf/F9j2MXg48FBH7kLTlvjydfjRJx2f7ALNJ2tRnc0s6v2HZnD2YmplZ22izxBERj5LcAJXpeJLO1khfT8iY/rNILAQGNnT53CAd7x8RT6ZXGT/LWN/MzNpJe1eO7x4RKwHS193S6cPZthOzGrbvk2g423Zwl22ZRpJmp/39V9fW1u504GZmlugolePZOoBrWneRzzJbZ0TMIenSgMrKyu2W27JlCzU1NWzevLmQOK0d9erVixEjRtC9e67ul8ysGNo7cbwlaY+IWJkWPb2dTq9h215BR5B0LZGphoyeUnMsk7eamhr69etHeXk5Sd28dSQRwerVq6mpqWH06NE7XsHM2k17F1XdR9I5GunrvRnT/zXtMfMgYF1DkVaDdHy9pIPSVlj/mrF+wTZv3syQIUOcNDooSQwZMsRXhGYtUFUF5eXQpUvyWlW1ozUK05bNcecChwJDJdWQ9Gb5beAuSV8k6bHyc+ni95M0xX2VpDnuGRnbeT4iJqaj57K1Oe7v0mFnYtyZ1a2N+e9jVriqKpg9G+rqkvE33kjGAWa1tD/kJtoscUTEzByztns2QdpK6vwc25mY8b4aGNsqAZqZ7YKuuGJr0mhQV5dMb63E4S5HimD16tVMnDiRiRMn8pGPfIThw4c3jr///vt5beOMM87gL3/5S7PL/PCHP6Sqta9RzTqpti7+aS3Lc3TdmGt6S3SUVlUdXlVVkrGXL4eRI+Gaa1qevYcMGcLzzz8PwNVXX03fvn35yle+ss0yjQ+F75I9t99+++073M/552e9iDOzArVH8U9rGTkyiS/b9NbiK448NPxo3ngDIrb+aFr7jOPVV19l7NixnHPOOVRUVLBy5Upmz55NZWUlY8aM4Rvf+EbjslOnTuX555+nvr6egQMHcvnllzNhwgQOPvhg3n47aax25ZVXcvPNNzcuf/nllzN58mQ+/vGP86c/JQ8+27hxI5/97GeZMGECM2fOpLKysjGpZbrqqqs44IADGuNr6Onlr3/9K5/+9KeZMGECFRUVLFu2DIBrr72WcePGMWHCBK644orW/aJsl1AqZ/DQfPFPR3PNNVBWtu20srJkeqtpOLPdlYf9998/mnr55Ze3m5bLqFERScrYdhg1Ku9N5HTVVVfFd7/73YiIWLp0aUiKp59+unH+6tWrIyJiy5YtMXXq1FiyZElEREyZMiWee+652LJlSwBx//33R0TEpZdeGt/61rciIuKKK66Im266qXH5r371qxERce+998aRRx4ZERHf+ta34rzzzouIiOeffz66dOkSzz333HZxNsTx4YcfxowZMxr3V1FREffdd19ERGzatCk2btwY9913X0ydOjXq6uq2WbclCvk7Wem4446IsrJt/5/KypLpHZGU/RggFTuy7O64Izk+SclrS75XoDpyHFN9xZGH9igzbLD33ntzwAEHNI7PnTuXiooKKioqeOWVV3j55Ze3W6d3794cffTRAOy///6NZ/1NnXTSSdst8/jjjzNjxgwAJkyYwJgxY7Ku+9BDDzF58mQmTJjAI488wpIlS1i7di2rVq3iM5/5DJDcsFdWVsYf/vAHzjzzTHr37g3A4MGDC/8ibJdWSmfwkLuYpzWLf1rTrFmwbBl8+GHy2trFaU4ceWjPH02fPn0a3y9dupTvfe97/PGPf2Tx4sUcddRRWe9r6NGjR+P7rl27Ul9fn3XbPXv23G6ZyONBXnV1dVxwwQXMmzePxYsXc+aZZzbGka3JbES4Ka01qz1PxlpDuxT/lBAnjjwU60fz7rvv0q9fP/r378/KlSv5/e9/3+r7mDp1KnfddRcAL774YtYrmk2bNtGlSxeGDh3K+vXrueeeewAYNGgQQ4cO5de//jWQ3FRZV1fH9OnTue2229i0aRMAa9Y07evSOrtSPIOfMwdGjQIpeZ0zp+NVjLcXJ448FOtHU1FRwX777cfYsWM5++yzmTJlSqvv48ILL+TNN99k/Pjx3HDDDYwdO5YBAwZss8yQIUM47bTTGDt2LCeeeCIHHnhg47yqqipuuOEGxo8fz9SpU6mtreXYY4/lqKOOorKykokTJ3LTTTe1etxW2krxDL6ti39KSad45nhlZWU0fZDTK6+8wr777lukiDqO+vp66uvr6dWrF0uXLmX69OksXbqUbt06Rktt/512Xa3ZxN1an6RFEVGZbV7HODpY0WzYsIHDDz+c+vp6IoIf/ehHHSZpWOFK6WA8a1bHjc2a5yNEJzdw4EAWLVpU7DCsFZTSTWpW2lzHYbaLKLUmrla6nDjMdhGl1sTVSpcTh9kuotSauFrpcuIw20WUYhNXK01OHEVy6KGHbndD380338x5553X7Hp9+/YFYMWKFZx88sk5t920+XFTN998M3UZBeLHHHMM77zzTj6hWwflm9SsvThxFMnMmTO58847t5l25513MnNmrudfbWvPPffk7rvvbvH+myaO+++/n4EDB7Z4e9Yx+CY1aw9OHEVy8skn85vf/Ib33nsPgGXLlrFixQqmTp3aeG9FRUUF48aN4957t3+0+rJlyxg7NnkY4qZNm5gxYwbjx4/nlFNOaezqA+Dcc89t7Jb9qquuAuD73/8+K1as4LDDDuOwww4DoLy8nFWrVgFw4403MnbsWMaOHdvYLfuyZcvYd999OfvssxkzZgzTp0/fZj8Nfv3rX3PggQcyadIkjjjiCN566y0guV/kjDPOYNy4cYwfP76x25L58+dTUVHBhAkTOPzw7R4OaWYdkO/jAC65BLI8gmKnTJwI6TE3qyFDhjB58mTmz5/P8ccfz5133skpp5yCJHr16sW8efPo378/q1at4qCDDuK4447L2XHgLbfcQllZGYsXL2bx4sVUVFQ0zrvmmmsYPHgwH3zwAYcffjiLFy/moosu4sYbb2TBggUMHTp0m20tWrSI22+/naeeeoqI4MADD+RTn/oUgwYNYunSpcydO5cf//jHfP7zn+eee+7h1FNP3Wb9qVOnsnDhQiRx6623ct1113HDDTfwzW9+kwEDBvDiiy8CsHbtWmprazn77LN59NFHGT16tPu0MisRvuIoosziqsxiqojg61//OuPHj+eII47gzTffbDxzz+bRRx9tPICPHz+e8ePHN8676667qKioYNKkSSxZsiRrJ4aZHn/8cU488UT69OlD3759Oemkk3jssccAGD16NBMnJo+Az9V9e01NDUceeSTjxo3ju9/9LkuWLAHgD3/4wzZPJBw0aBALFy5k2rRpjB49GuiY3a+X0sOGzNpLUa44JF0MnA0I+HFE3CzpF8DH00UGAu9ExMQs6y4D1gMfAPW5+lIpRHNXBm3phBNO4LLLLuPZZ59l06ZNjVcKVVVV1NbWsmjRIrp37055eXnW7tQzZbsaef3117n++ut55plnGDRoEKeffvoOt9Nc32UN3bJD0jV7tqKqCy+8kMsuu4zjjjuOhx9+mKuvvrpxu01j7Ojdr/tObLPs2v2KQ9JYkqQxGZgAHCtpn4g4JSImpsniHuCXzWzmsHTZnU4axdS3b18OPfRQzjzzzG0qxdetW8duu+1G9+7dWbBgAW9ke4BwhmnTplGVngq/9NJLLF68GEi6Ze/Tpw8DBgzgrbfe4ne/+13jOv369WP9+vVZt/WrX/2Kuro6Nm7cyLx58zjkkEPy/kzr1q1j+PDhAPz0pz9tnD59+nR+8IMfNI6vXbuWgw8+mEceeYTXX38d6Hjdr/tObLPsilFUtS+wMCLqIqIeeAQ4sWGmklPQzwNzixBbu5s5cyYvvPBC41P4AGbNmkV1dTWVlZVUVVXxiU98otltnHvuuWzYsIHx48dz3XXXMXnyZCB5ot+kSZMYM2YMZ5555jbdss+ePZujjz66sXK8QUVFBaeffjqTJ0/mwAMP5KyzzmLSpEl5f56rr76az33ucxxyyCHb1J9ceeWVrF27lrFjxzJhwgQWLFjAsGHDmDNnDieddBITJkzglFNOyXs/7cF3Yptl1+7dqkvaF7gXOBjYBDxE8mzbC9P504Abc11NSHodWAsE8KOImJNjudnAbICRI0fu3/Ss3d11l4Zi/p3Ky5PiqaZGjUqauprtyprrVr3drzgi4hXgO8CDwHzgBSDzWaczaf5qY0pEVABHA+eniSbbfuZERGVEVA4bNqx1grdOxXdim2VXlFZVEXFbRFRExDRgDbAUQFI34CTgF82suyJ9fRuYR1JXYtbqfCe2WXbFalW1W0S8LWkkSaI4OJ11BPDniKjJsV4foEtErE/fTwe+0dI4Onqrns6uIzyd0g8bMttese7juEfSy8CvgfMjYm06fQZNiqkk7Snp/nR0d+BxSS8ATwO/jYj5LQmgV69erF69ukMcnGx7EcHq1avp1atXsUMxsyY67TPHt2zZQk1NzQ7va7Di6dWrFyNGjKB79+7FDsWs0/Ezx7Po3r174x3LZmaWP3c5YmZmBXHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRh7c6PYzUrbZ32znErDj+O1az0+YrD2pUfx2pW+pw4rF35caxmpc+Jw9rVyJGFTTezjseJw9qVH8dqVvqcOKxd+XGsZqXPraqs3flxrGalzVccZmZWECcOMzMrSFESh6SLJb0kaYmkS9JpV0t6U9Lz6XBMjnWPkvQXSa9Kurx9Izczs3av45A0FjgbmAy8D8yX9Nt09k0RcX0z63YFfgj8E1ADPCPpvoh4uY3DNjOzVDGuOPYFFkZEXUTUA48AJ+a57mTg1Yh4LSLeB+4Ejm+jOM3MLItiJI6XgGmShkgqA44B9krnXSBpsaSfSBqUZd3hwN8zxmvSaduRNFtStaTq2tra1ozfzKxTa/fEERGvAN8BHgTmAy8A9cAtwN7ARGAlcEOW1ZVtkzn2MyciKiOictiwYa0RupmZUaTK8Yi4LSIqImIasAZYGhFvRcQHEfEh8GOSYqmmath6dQIwAljR9hGbmVmDYrWq2i19HQmcBMyVtEfGIieSFGk19Qywj6TRknoAM4D72jpeMzPbqlh3jt8jaQiwBTg/ItZK+m9JE0mKnpYBXwKQtCdwa0QcExH1ki4Afg90BX4SEUuK8xHMzDqnoiSOiDgky7R/ybHsCpIK9Ibx+4H72y46MzNrju8cNzOzgjhxmJlZQZw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgThxmZlYQJw4zMyuIE4eZmRXEicPMzArixGFmZgVx4jAzs4I4cZiZWUGcOHYBVVVQXg5duiSvVVXFjsjMdmVFeea4tZ6qKpg9G+rqkvE33kjGAWbNKl5cZrbrKsoVh6SLJb0kaYmkS9Jp35X0Z0mLJc2TNDDHusskvSjpeUnV7Rt5x3PFFVuTRoO6umS6mVlbaPfEIWkscDYwGZgAHCtpH+BBYGxEjAf+Cvx7M5s5LCImRkRlmwfcwS1fXth0M7OdtcPEIekCSYNacZ/7Agsjoi4i6oFHgBMj4oF0HGAhMKIV97nLGjmysOlmZjsrnyuOjwDPSLpL0lGStJP7fAmYJmmIpDLgGGCvJsucCfwux/oBPCBpkaTZuXYiabakaknVtbW1Oxlyx3XNNVBWtu20srJkuplZW9hh4oiIK4F9gNuA04Glkq6VtHdLdhgRrwDfISmamg+8ADRcaSDpinQ8V9ugKRFRARwNnC9pWo79zImIyoioHDZsWEtCLQmzZsGcOTBqFEjJ65w5rhg3s7aTVx1HRATwj3SoBwYBd0u6riU7jYjbIqIiIqYBa4ClAJJOA44FZqX7zLbuivT1bWAeSV1JpzZrFixbBh9+mLw6aZhZW8qnjuMiSYuA64AngHERcS6wP/DZluxU0m7p60jgJGCupKOArwHHRURdjvX6SOrX8B6YTlL0ZWZm7SSf+ziGAidFxBuZEyPiQ0nHtnC/90gaAmwBzo+ItZJ+APQEHkyrURZGxDmS9gRujYhjgN2Been8bsDPI2J+C2MwM7MWyCdx3E9SnARAesa/X0Q8ldZXFCwiDsky7WM5ll1BUoFORLxG0oTXzMyKJJ86jluADRnjG9NpZmbWCeWTOJRZUR0RH+KuSszMOq18EsdraQV593S4GHitrQMzM7OOKZ/EcQ7wSeBNoAY4EMh5452Zme3adljklN4vMaMdYjEzsxKww8QhqRfwRWAM0KthekSc2YZxmZlZB5VPUdV/k/RXdSRJh4QjgPVtGZSZmXVc+SSOj0XEfwAbI+KnwD8D49o2LDMz66jySRxb0td30mdpDADK2ywiMzPr0PK5H2NO+jyOK4H7gL7Af7RpVGZm1mE1mzgkdQHejYi1wKPAR9slKjMz67CaLapK7xK/oJ1iMTOzEpBPHceDkr4iaS9JgxuGNo/MzMw6pHzqOBru1zg/Y1rgYiszs04pnzvHR7dHIGZmVhryuXP8X7NNj4iftX44ZmbW0eVTVHVAxvtewOHAs4ATh5lZJ5RPUdWFmeOSBpB0Q2JmZp1QPq2qmqoD9mntQMzMrDTsMHFI+rWk+9LhN8BfgHt3ZqeSLpb0kqQlki5Jpw2W9KCkpenroBzrnpYus1TSaTsTh5mZFS6fOo7rM97XA29ERE1Ld5j2d3U2MBl4H5gv6bfptIci4tuSLgcuB77WZN3BwFVAJUmT4EWS7kvvbDczs3aQT1HVcuCpiHgkIp4AVksq34l97gssjIi6iKgn6ar9ROB44KfpMj8FTsiy7pHAgxGxJk0WDwJH7UQsZmZWoHwSx/8AH2aMf5BOa6mXgGmShkgqA44B9gJ2j4iVAOnrblnWHQ78PWO8Jp22HUmzJVVLqq6trd2JcM3MLFM+iaNbRLzfMJK+79HSHUbEK8B3SK4W5gMvkBSB5UPZNpljP3MiojIiKocNG9aiWM3MbHv5JI5aScc1jEg6Hli1MzuNiNsioiIipgFrgKXAW5L2SPexB/B2llVrSK5OGowAVuxMLGZmVph8Esc5wNclLZe0nKTC+ks7s1NJu6WvI4GTgLkkz/poaCV1Gtlbbv0emC5pUNrqano6zczM2kk+NwD+DThIUl9AEdEazxu/R9IQkqcLnh8RayV9G7hL0hdJKuQ/ByCpEjgnIs6KiDWSvgk8k27nGxGxphXiMTOzPCkiaxXB1gWka4HrIuKddHwQ8OWIuLId4msVlZWVUV1dXewwzMxKhqRFEVGZbV4+RVVHNyQNgLQZ7DGtFZyZmZWWfG4A7CqpZ0S8ByCpN9CzbcMyM7N8RMD69bBq1bZDbW0y7ytfaf195pM47gAeknR7On4GW2/UM9ulvf8+rFsHGzcWO5LC9OgBvXolQ8+e0LVrsSOyfG3evH0CaJoUms7bsiX7tnbbrUiJIyKuk7QYOILkPor5wKjWD8Ws9dXXJwf+d96BtWuT1+aGpsvU1RX7E7SO7t23JpJ8h969C18n29CzJ3RpSXequ4D6elizpvmDf9Pk0NxJyuDBMGwYDB0Ko0fDAQck7xuGhnkNQ//+bfO58rniAPgHyd3jnwdeB+5pm3DMtvXBB1sP/IUc8BuGDRua336XLjBwYDIMGpS87rHH1mkNQ58+oGy3n3ZAEckZ6ObN+Q/r1ycHr1zzd1aPHrkTUY8eSWLr1q39h0L326VL8nvc0cG/YVjbTC96fftuPdDvthvst1/uBDB0aPL77JbvEbuN5QxD0v8CZgAzgdXAL0haYR3WTrHZLiIC3n03+z/Vjq4C3n23+W1L2x/k99ln2/GGhJBt6Nu3dBJCsUQkRXb5JqFNmwpLWps3J9uvq0vO0FsyFFuPHtse7Csqmk8CQ4YkSbNUNZe//gw8BnwmIl4FkHRpu0RlHdqmTfmdbWVOb+6fe8CAbQ/mo0fv+IDfMPTr13mLQdqLlBQ39eyZ/K06mgj48MPCEs2WLS1PUgMGbJ8IOtsJSHOJ47MkVxwLJM0H7iR7X1FWwurrYfXqwhJBrnJ/KTmTavhn2ntvOPDA7GddQ4YkQ79+rri1nSMlv6GuXZPkZm0vZ+KIiHnAPEl9SLo4vxTYXdItwLyIeKCdYrQ8RSTFO/mWv+6oDLZ//60H+t13hzFjcl96N5TBOgmY7fryaVW1EagCqtIHKX2O5CFLThxF9u678Nhj8PDDsGABvPBC7iKhnj23PeCPGtV8Ehg6NCm3NTNrqqA6+rRfqB+lg7Wz9evh8ce3JopFi5Ky3R494OCD4ctfho98ZPsEMGwYlJV1rjJYM2s7HaRxl2WzYQM88USSJB5+GKqrk+ap3bvDQQfBFVfAYYcl73v3Lna0ZtZZOHF0IBs3wp/+tDVRPPNMUvTUrVtSyXz55UmiOPjg5ArCzKwYnDiKqK4Onnxya1RhfCgAAAyBSURBVKJ4+umkmWC3bskdof/2b0mi+OQnkxvQzMw6AieOdrRpU5IoGuoonnoqSRRdu0JlJVx2WZIopkxJ2oWbmXVEThxtaPNmWLhwa6JYuDC5Q7ZLF9h/f7jkkq2Joq36lDEza21OHK3ovfeSq4iGRPHkk8m0Ll1g0iS48MIkUUyd2jHvwDUzy4cTx054//2kXqIhUfzpT8lVhgQTJ8J55yWJ4pBDku4xzMx2BU4cBdiyJWnp1JAonngiqbcAmDABvvSlrYli8OCihmpm1macOJqxZUtyk11Dq6fHH9/aT9O4cXDWWUmimDYt6XfJzKwzKEriSHvZPQsI4EWSpwo+CPRLF9kNeDoiTsiy7gfpOgDLI+K4tohx8+bkLux165LxMWPgzDPh0EOTRDFsWFvs1cys42v3xCFpOHARsF9EbJJ0FzAjIg7JWOYe4N4cm9gUERPbOs5evZIb7vbeGz71qeRBK2ZmVryiqm5Ab0lbgDJgRcMMSf2AT5NchRTV5ZcXOwIzs46n3R+BExFvAtcDy4GVwLomXbSfCDwUEbme/dZLUrWkhZK2K8pqIGl2ulx1bW1tq8VvZtbZtXvikDQIOB4YDewJ9JF0asYiM4G5zWxiZERUAl8Abpa0d7aFImJORFRGROUwV0iYmbWaYjx08wjg9YiojYgtwC+BTwJIGgJMBn6ba+WIWJG+vgY8DExq64DNzGyrYiSO5cBBksokCTgceCWd9zngNxGxOduKkgZJ6pm+HwpMAV5uh5jNzCxVjDqOp4C7gWdJmtV2Aeaks2fQpJhKUqWkW9PRfYFqSS8AC4BvR4QTh5lZO1JEFDuGNldZWRnV1dXFDsPMrGRIWpTWJ2+nGEVVZmZWwpw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgThxmZlYQJw4zMyuIE4eZmRXEicPMzArixGFmZgVx4jAzs4I4cZiZWUGcOMzMrCBOHGZmVpCiJA5Jl0paIuklSXMl9ZL0X5Jel/R8OkzMse5pkpamw2ntHbuZWWfXrb13KGk4cBGwX0RsknQXMCOd/W8RcXcz6w4GrgIqgQAWSbovIta2ddxmZpYoVlFVN6C3pG5AGbAiz/WOBB6MiDVpsngQOKqNYjQzsyzaPXFExJvA9cByYCWwLiIeSGdfI2mxpJsk9cyy+nDg7xnjNem07UiaLalaUnVtbW0rfgIzs86t3ROHpEHA8cBoYE+gj6RTgX8HPgEcAAwGvpZt9SzTItt+ImJORFRGROWwYcNaJXYzMytOUdURwOsRURsRW4BfAp+MiJWReA+4HZicZd0aYK+M8RHkX8xlZmatoBiJYzlwkKQySQIOB16RtAdAOu0E4KUs6/4emC5pUHrlMj2dZmZm7aTdW1VFxFOS7gaeBeqB54A5wO8kDSMpjnoeOAdAUiVwTkScFRFrJH0TeCbd3DciYk17fwYzs85MEVmrCHYplZWVUV1dXewwzMxKhqRFEVGZbZ7vHDczs4I4cZiZWUGcOMzMrCBOHGZmVhAnDjMzK4gTh5mZFcSJw8zMCuLEYWZmBXHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgThxmZlaQoiQOSZdKWiLpJUlzJfWSVCXpL+m0n0jqnmPdDyQ9nw73tXfsZmadXbsnDknDgYuAyogYC3QFZgBVwCeAcUBv4Kwcm9gUERPT4bj2iNnMzLbqVsT99pa0BSgDVkTEAw0zJT0NjChSbGZm1ox2v+KIiDeB64HlwEpgXZOk0R34F2B+jk30klQtaaGkE3LtR9LsdLnq2traVvwEZmadWzGKqgYBxwOjgT2BPpJOzVjk/wCPRsRjOTYxMiIqgS8AN0vaO9tCETEnIiojonLYsGEFx1lVBeXl0KVL8lpVVfAmzMx2ScWoHD8CeD0iaiNiC/BL4JMAkq4ChgGX5Vo5Ilakr68BDwOTWjvAqiqYPRveeAMiktfZs508zMygOIljOXCQpDJJAg4HXpF0FnAkMDMiPsy2oqRBknqm74cCU4CXWzvAK66Aurptp9XVJdPNzDq7YtRxPAXcDTwLvJjGMAf4v8DuwJNpU9v/DSCpUtKt6er7AtWSXgAWAN+OiFZPHMuXFzbdzKwzUUQUO4Y2V1lZGdXV1XkvX16eFE81NWoULFvWamGZmXVYkhal9cnb8Z3jWVxzDZSVbTutrCyZbmbW2TlxZDFrFsyZk1xhSMnrnDnJdDOzzq5YNwB2eLNmOVGYmWXjKw4zMyuIE4eZmRXEicPMzArixGFmZgVx4jAzs4J0ihsAJdUCWW7py8tQYFUrhtOWSilWKK14SylWKK14SylWKK14dybWURGRtYfYTpE4doak6lx3T3Y0pRQrlFa8pRQrlFa8pRQrlFa8bRWri6rMzKwgThxmZlYQJ44dm1PsAApQSrFCacVbSrFCacVbSrFCacXbJrG6jsPMzAriKw4zMyuIE4eZmRXEiSMHST+R9Lakl4ody45I2kvSAkmvSFoi6eJix5SLpF6Snpb0Qhrr/1fsmPIhqauk5yT9ptixNEfSMkkvpk/RzP/pZUUiaaCkuyX9Of39HlzsmLKR9PH0O20Y3pV0SbHjao6kS9P/sZckzZXUq9W27TqO7CRNAzYAP4uIscWOpzmS9gD2iIhnJfUDFgEntMVjdXdW+pz5PhGxQVJ34HHg4ohYWOTQmiXpMqAS6B8RxxY7nlwkLQMqI6IkblCT9FPgsYi4VVIPoCwi3il2XM2R1BV4EzgwIlp6Y3GbkjSc5H9rv4jYJOku4P6I+K/W2L6vOHKIiEeBNcWOIx8RsTIink3frwdeAYYXN6rsIrEhHe2eDh367EXSCOCfgVuLHcuuRFJ/YBpwG0BEvN/Rk0bqcOBvHTVpZOgG9JbUDSgDVrTWhp04djGSyoFJwFPFjSS3tNjneeBt4MGI6LCxpm4Gvgp8WOxA8hDAA5IWSZpd7GB24KNALXB7Wgx4q6Q+xQ4qDzOAucUOojkR8SZwPbAcWAmsi4gHWmv7Thy7EEl9gXuASyLi3WLHk0tEfBARE4ERwGRJHbYoUNKxwNsRsajYseRpSkRUAEcD56dFrh1VN6ACuCUiJgEbgcuLG1Lz0uK044D/KXYszZE0CDgeGA3sCfSRdGprbd+JYxeR1hfcA1RFxC+LHU8+0mKJh4GjihxKc6YAx6V1B3cCn5Z0R3FDyi0iVqSvbwPzgMnFjahZNUBNxhXn3SSJpCM7Gng2It4qdiA7cATwekTURsQW4JfAJ1tr404cu4C0wvk24JWIuLHY8TRH0jBJA9P3vUl+4H8ublS5RcS/R8SIiCgnKaL4Y0S02plba5LUJ20cQVrkMx3osK0CI+IfwN8lfTyddDjQ4Rp0NDGTDl5MlVoOHCSpLD0+HE5S99kqnDhykDQXeBL4uKQaSV8sdkzNmAL8C8nZcENzwWOKHVQOewALJC0GniGp4+jQTVxLyO7A45JeAJ4GfhsR84sc045cCFSlv4eJwLVFjicnSWXAP5GcvXdo6VXc3cCzwIskx/pW637EzXHNzKwgvuIwM7OCOHGYmVlBnDjMzKwgThxmZlYQJw4zMyuIE4dZC0n6oEmPqa1217Ok8lLomdk6p27FDsCshG1Ku04x61R8xWHWytJnYnwnfe7I05I+lk4fJekhSYvT15Hp9N0lzUufUfKCpIauIbpK+nH6TIUH0jvtkXSRpJfT7dxZpI9pnZgTh1nL9W5SVHVKxrx3I2Iy8AOS3nVJ3/8sIsYDVcD30+nfBx6JiAkkfTUtSafvA/wwIsYA7wCfTadfDkxKt3NOW304s1x857hZC0naEBF9s0xfBnw6Il5LO5/8R0QMkbSK5IFbW9LpKyNiqKRaYEREvJexjXKS7lj2Sce/BnSPiP+UNJ/kIWO/An6V8XwTs3bhKw6zthE53udaJpv3Mt5/wNY6yX8GfgjsDyxKH9Rj1m6cOMzaxikZr0+m7/9E0sMuwCySR3sCPAScC40Pueqfa6OSugB7RcQCkodLDQS2u+oxa0s+UzFrud7pkwwbzI+Ihia5PSU9RXJyNjOddhHwE0n/RvLkuzPS6RcDc9IemD8gSSIrc+yzK3CHpAGAgJtK5HGrtgtxHYdZK0vrOCojYlWxYzFrCy6qMjOzgviKw8zMCuIrDjMzK4gTh5mZFcSJw8zMCuLEYWZmBXHiMDOzgvw/jNGxCCFtl8gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "y_pred_list = []    # store predicted values for confusion matrix\n",
    "y_target_list = []  # ground truth value\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "    \n",
    "    # store predicted values and ground truth values for calculating confusion matrix\n",
    "    y_pred_list.extend((y_pred>0.5).cpu().long().numpy())\n",
    "    y_target_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.267\n",
      "Test Accuracy: 89.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "rating_classes = []\n",
    "for i in range(len(dataset._vectorizer.rating_vocab)):\n",
    "    rating_classes.append(dataset._vectorizer.rating_vocab.lookup_index(i))\n",
    "print(rating_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True       negative  positive\n",
      "Predicted                    \n",
      "negative        750        84\n",
      "positive         83       747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "cm = confusion_matrix(y_target_list, y_pred_list)\n",
    "cm_df = pd.DataFrame(cm.T, index=rating_classes, columns=rating_classes)\n",
    "cm_df.index.name = 'Predicted'\n",
    "cm_df.columns.name = 'True'\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       833\n",
      "           1       0.90      0.90      0.90       831\n",
      "\n",
      "    accuracy                           0.90      1664\n",
      "   macro avg       0.90      0.90      0.90      1664\n",
      "weighted avg       0.90      0.90      0.90      1664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_target_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create my own test loop\n",
    "# def loop(tuningPara):\n",
    "#     args = Namespace(\n",
    "#     # Data and Path information\n",
    "#     frequency_cutoff=25,\n",
    "#     model_state_file='model.pth',\n",
    "#     review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "#     save_dir='model_storage/ch3/yelp/',\n",
    "#     # No Model hyper parameters\n",
    "#     hidden_dim=20,\n",
    "#     # Training hyper parameters\n",
    "#     batch_size=128,\n",
    "#     early_stopping_criteria=5,\n",
    "#     learning_rate=0.001,\n",
    "#     num_epochs=100,\n",
    "#     seed=1337,\n",
    "#     # Runtime options\n",
    "#     catch_keyboard_interrupt=True,\n",
    "#     cuda=True,\n",
    "#     expand_filepaths_to_save_dir=True,\n",
    "#     reload_from_files=False,\n",
    "#     )\n",
    "\n",
    "#     if args.expand_filepaths_to_save_dir:\n",
    "#         args.model_state_file = os.path.join(args.save_dir,\n",
    "#                                              args.model_state_file)    \n",
    "#     #    print(\"Expanded filepaths: \")\n",
    "#     #    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "#     # Check CUDA\n",
    "#     if not torch.cuda.is_available():\n",
    "#         args.cuda = False\n",
    "\n",
    "#     #print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "#     args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#     # Set seed for reproducibility\n",
    "#     set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "#     # handle dirs ; creat dirs if they don't exist\n",
    "#     handle_dirs(args.save_dir)\n",
    "    \n",
    "#     ##\n",
    "#     print(\"Loading dataset and creating vectorizer\")\n",
    "# # create dataset and vectorizer\n",
    "#     dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv, args.frequency_cutoff) \n",
    "\n",
    "#     vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "#     classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab), hidden_dim=args.hidden_dim)\n",
    "#     ##\n",
    "#     classifier = classifier.to(args.device)\n",
    "\n",
    "#     loss_func = nn.BCELoss()\n",
    "#     optimizer = optim.Adam(classifier.parameters(), lr=tuningPara) #tune from 0.001\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "#                                                      mode='min', factor=0.1,\n",
    "#                                                      patience=10) # Reduce learning rate when a metric has stopped improving.\n",
    "\n",
    "#     train_state = make_train_state(args)\n",
    "\n",
    "#     epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)  # progress bar\n",
    "\n",
    "#     dataset.set_split('train')\n",
    "#     train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "#     dataset.set_split('val')\n",
    "#     val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "#     try:\n",
    "#         for epoch_index in range(args.num_epochs):\n",
    "#             train_state['epoch_index'] = epoch_index\n",
    "\n",
    "#             # Iterate over training dataset\n",
    "\n",
    "#             # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "#             dataset.set_split('train')\n",
    "#             batch_generator = generate_batches(dataset, \n",
    "#                                                batch_size=args.batch_size, \n",
    "#                                                device=args.device)\n",
    "#             running_loss = 0.0\n",
    "#             running_acc = 0.0\n",
    "#             classifier.train()\n",
    "\n",
    "#             for batch_index, batch_dict in enumerate(batch_generator):\n",
    "#                 # the training routine is these 5 steps:\n",
    "\n",
    "#                 # --------------------------------------\n",
    "#                 # step 1. zero the gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # step 2. compute the output\n",
    "#                 y_pred = classifier(x_in=batch_dict['x_data'].float())  # [batch, num_features] -> [batch]\n",
    "\n",
    "#                 # step 3. compute the loss\n",
    "#                 loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "#                 loss_t = loss.item()\n",
    "#                 running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "#                 # step 4. use loss to produce gradients\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # step 5. use optimizer to take gradient step\n",
    "#                 optimizer.step()\n",
    "#                 # -----------------------------------------\n",
    "#                 # compute the accuracy\n",
    "#                 acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "#                 running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "#                 # update bar\n",
    "#                 train_bar.set_postfix(loss=running_loss, \n",
    "#                                       acc=running_acc, \n",
    "#                                       epoch=epoch_index)\n",
    "#                 train_bar.update()\n",
    "\n",
    "#             train_state['train_loss'].append(running_loss)  # train_loss for each epoch\n",
    "#             train_state['train_acc'].append(running_acc)    # train_acc for each epoch\n",
    "\n",
    "#             # Iterate over val dataset\n",
    "\n",
    "#             # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "#             dataset.set_split('val')\n",
    "#             batch_generator = generate_batches(dataset, \n",
    "#                                                batch_size=args.batch_size, \n",
    "#                                                device=args.device)\n",
    "#             running_loss = 0.\n",
    "#             running_acc = 0.\n",
    "#             classifier.eval()\n",
    "\n",
    "#             for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "#                 # compute the output\n",
    "#                 y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "#                 # step 3. compute the loss\n",
    "#                 loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "#                 loss_t = loss.item()\n",
    "#                 running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "#                 # compute the accuracy\n",
    "#                 acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "#                 running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "#                 val_bar.set_postfix(loss=running_loss, \n",
    "#                                     acc=running_acc, \n",
    "#                                     epoch=epoch_index)\n",
    "#                 val_bar.update()\n",
    "\n",
    "#             train_state['val_loss'].append(running_loss)  # val_loss for each epoch\n",
    "#             train_state['val_acc'].append(running_acc)    # val_acc for each epoch\n",
    "\n",
    "#             train_state = update_train_state(args=args, model=classifier,\n",
    "#                                              train_state=train_state)\n",
    "\n",
    "#             scheduler.step(train_state['val_loss'][-1])  # adjust learning rate\n",
    "\n",
    "#             train_bar.n = 0  # Number of finished iterations\n",
    "#             val_bar.n = 0\n",
    "#             epoch_bar.update()\n",
    "\n",
    "#             if train_state['stop_early']:\n",
    "#                 break\n",
    "\n",
    "#             train_bar.n = 1   # reset number of finished iterations\n",
    "#             val_bar.n = 1\n",
    "#             epoch_bar.update()\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Exiting loop\")\n",
    "#     # compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "#     classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "#     classifier = classifier.to(args.device)\n",
    "\n",
    "#     dataset.set_split('test')\n",
    "#     batch_generator = generate_batches(dataset, \n",
    "#                                        batch_size=args.batch_size, \n",
    "#                                        device=args.device)\n",
    "#     running_loss = 0.\n",
    "#     running_acc = 0.\n",
    "#     classifier.eval()\n",
    "\n",
    "#     y_pred_list = []    # store predicted values for confusion matrix\n",
    "#     y_target_list = []  # ground truth value\n",
    "\n",
    "#     for batch_index, batch_dict in enumerate(batch_generator):\n",
    "#         # compute the output\n",
    "#         y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "#         # store predicted values and ground truth values for calculating confusion matrix\n",
    "#         y_pred_list.extend((y_pred>0.5).cpu().long().numpy())\n",
    "#         y_target_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "\n",
    "#         # compute the loss\n",
    "#         loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "#         loss_t = loss.item()\n",
    "#         running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "#         # compute the accuracy\n",
    "#         acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "#         running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "#     train_state['test_loss'] = running_loss\n",
    "#     train_state['test_acc'] = running_acc\n",
    "#     print(f\"Learning Rate: {tuningPara}\")\n",
    "#     print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "#     print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n",
    "#     return running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currentAccuracy = 0\n",
    "# previousAccuracy = 87.92\n",
    "# tuningRate = 0.000001\n",
    "# startingValue = 0.001\n",
    "# for x in range(1,9):\n",
    "#     currentAccuracy = loop(startingValue + tuningRate*3)\n",
    "#     print(\"Accuracy to beat: 87.92\")\n",
    "#     print(\"Current Accuracy: \",currentAccuracy)\n",
    "#     print(\"Previous Accuracy: \",previousAccuracy)\n",
    "    \n",
    "#     if (currentAccuracy < previousAccuracy):\n",
    "#         tuningRate = tuningRate * (-1) #invert the tuning\n",
    "#     previousAccuracy  = currentAccuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
